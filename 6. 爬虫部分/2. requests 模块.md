# requests 模块

## 1. request的请求方法及携带的参数

```python
# get方法：

   1. 请求的方式：get
    
   2. 请求的地址： url
 
   3. 请求的头信息：headers:
    
         referer:判断用户是从哪一个链接条转过来的。
         user-agent:用户代理，判断用户使用什么发起请求的。
    
   4.请求体：只有当请求方式为post，才有请求体
         
    5. 响应头：
         cookie信息
         location(页面的跳转)：当状态码为3xx时可能会有
      
    6.响应体：  prever的源代码
         HTML代码(re.text)：---> 用re或者beautifulsoup模块解析
         二进制格式(re.content)：---> 直接写入文件 
         json格式：————> 反序列化
        
                                             
   # post方法：

   1. 请求的方式：post
    
   2. 请求的地址： url
 
   3. 请求的头信息：headers:
         cookies:用户的验证信息
         referer:判断用户是从哪一个链接条转过来的。
         user-agent:用户代理，判断用户使用什么发起请求的。
    
   4.请求体：data: ---> 在浏览器上的form-data
    
          'commit': 'Sign in',
          'utf8': '✓',
          'authenticity_token': token,
           'login':'378533872@qq.com',
           'password':'lhf@123'
         
         
    5. 响应头：
         cookie信息
         location(页面的跳转)：当状态码为3xx时可能会有
          
      
    6.响应体：prever的源代码
    
         HTML代码(re.text)：---> 用re或者beautifulsoup模块解析 ---> 可能存在一些被                                                   隐藏掉的token信息(scrf-token)
        
         二进制格式(re.content)：---> 直接写入文件 
        
         json格式：————> 反序列化
      
     
```

## 2. 其他的方法及注意点

```python 
# 1. 响应的状态码：
   
   200：成功
   300：重定向
   400：客户端有问题
   500：服务器问题

# 2.response.text：返回的是HTML格式的字符串格式(可以利re/beautifulsoup模块对数据筛选)

# 3.response.content：返回的是bytes格式的数据(比如视频，可以直接写入文件)

# 4.response.json:返回的是json的格式，直接反序列化就可以得到字典或则列表格式的数据

# 5.设置浏览器的重定向的参数：allow_redirtects =false  --> 禁止页面的重定向。

# 6.数据编码格式的注意点：
    
       // 在以请求方式为get的时候，url后面要加中文参数的时候：
        
        # 方法一：
        from urllib.parse import urlencode
        kwd = input('请输入要搜索的内容：').strip()
        res = urlencode({'wd':kwd})
        print(res)
        '''
        请输入要搜索的内容：美女
        wd=%E7%BE%8E%E5%A5%B3    
        '''
        
        # 方式二：(params参数)
        
kwd = input('请输入要搜索的内容：').strip()
response= requests.get(url ='https://www.baidu.com/s',params={'wd':kwd},
                       headers={
                           "user-agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36"},
                       )

print('basketball' in response.text)
    
    
 #7:cookie 和 token存放的地方
   
     cookie：可以存放在请求头里headers中，也可以放在cookies参数中
     
      token：放在请求体中：data
 
# 8.得到浏览器给的cookie信息的方法：
    
    response.cookies.get_dict()

```

## 3. 其他高级的用法

```python 
# 1.#证书验证(大部分网站都是https)

import requests
respone=requests.get('https://www.12306.cn') #如果是ssl请求,首先检查证书是否合法,不合法则报错,程序终端


#改进1:去掉报错,但是会报警告
import requests
respone=requests.get('https://www.12306.cn',verify=False) #不验证证书,报警告,返回200
print(respone.status_code)


#改进2:去掉报错,并且去掉警报信息
import requests
from requests.packages import urllib3
urllib3.disable_warnings() #关闭警告
respone=requests.get('https://www.12306.cn',verify=False)
print(respone.status_code)

#改进3:加上证书
#很多网站都是https,但是不用证书也可以访问,大多数情况都是可以携带也可以不携带证书
#知乎\百度等都是可带可不带
#有硬性要求的,则必须带，比如对于定向的用户,拿到证书后才有权限访问某个特定网站
import requests
respone=requests.get('https://www.12306.cn',
                     cert=('/path/server.crt',
                           '/path/key'))
print(respone.status_code)SSL Cert Verification

#############################################################################

# 2.使用代理(proxies,socks)：代理设置:先发送请求给代理,然后由代理帮忙发送(封ip是常见的事情)

    import requests
proxies={
    'http':'http://egon:123@localhost:9743',#带用户名密码的代理,@符号前是用户名与密码
    'http':'http://localhost:9743',
    'https':'https://localhost:9743',
}
respone=requests.get('https://www.12306.cn',
                     proxies=proxies)

print(respone.status_code)


#支持socks代理,安装:pip install requests[socks]
import requests
proxies = {
    'http': 'socks5://user:pass@host:port',
    'https': 'socks5://user:pass@host:port'
}
respone=requests.get('https://www.12306.cn',
                     proxies=proxies)

print(respone.status_code)
   
#############################################################################

# 3. 超时设置：timeout
   import requests
    respone=requests.get('https://www.baidu.com',
                     timeout=0.0001)
    
#############################################################################

# 4. 认证设置(了解)

# 5. 异常处理(try)

#############################################################################

# 6. 上传文件
  import requests
	files={'file':open('a.jpg','rb')}
	respone=requests.post('http://httpbin.org/post',files=files)
	print(respone.status_code)

 #####################################################################
 
 # 7.指定响应体编码:response.encoding

    import requests
    response=requests.get('http://www.autohome.com/news')
    response.encoding = "GBK"
    print(response.text)
    print(response.encoding)
```

## 4. 基于get方法获取视频

```python
''''
步骤：
1. 访问网页的主页面，返回整个页面的HTML
2. 根据返回的HTML，过滤出各个视屏的连接
3. 通过得到的各个视屏的连接，返回视屏的HTML
4. 通过返回的是视屏的HTML，过滤出视屏的详细信息
'''

import  requests
import  re
import  os
from concurrent.futures import  ThreadPoolExecutor

base_url = 'https://www.pearvideo.com/'

#1. 访问网页的主页面，返回整个页面的HTML
def get_index():
    res = requests.get(base_url,headers={
       "user - agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36"
    })
    return res.text


#2. 根据返回的HTML，过滤出各个视屏的连接
def get_detail(index_text ):
    urls = re.findall('<a href="(.*?)" class="vervideo-lilink actplay">',index_text)
    return  urls

#3. 通过得到的各个视频的链接，对其进行访问并会页面的html代码
def get_datil_index(url):
    res = requests.get(url, headers={
        "user - agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36"
    })
    return res.text


# 4. 通过返回的是视屏的HTML，过滤出视屏的详细信息
def get_video(detail_index_text):

    #视频的链接
    url_video = re.search('srcUrl="(.*?\.mp4)',detail_index_text).group(1)
    # 标题
    title= re.search('<h1 class="video-tt">(.*?)</h1>',detail_index_text).group(1)
    # 内容
    content = re.search('<div class="summary">(.*?)</div>',detail_index_text).group(1)
    # 时间
    date =  re.search('<div class="date">(.*?)</div>',detail_index_text).group(1)
    # 点赞数
    upper_count =re.search('<div class="fav" data-id=".*?">(.*?)</div>',detail_index_text).group(1)

    return {'视频的链接':url_video,'标题':title,'内容':content,'发布时间':date,'点赞数':upper_count}



# 将视频下载到本地
def download_video(video_url,title):
    # 访问视频的链接
    data = requests.get(video_url)
    if not os.path.exists('video'):
        os.makedirs('video')
    title=title.replace(':',"_")
    file_path = os.path.join('video',title)+'.mp4'
    with open(file_path,'wb')as f:
        # 将视频二进制的格式存入文件
        f.write(data.content)
    print('下载成功')

if __name__ == '__main__':
    pool = ThreadPoolExecutor(5)
    index_text = get_index()     # 返回的是主页的HTML代码
    urls_list=get_detail(index_text) # 返回的是视频详情的链接
    for url in urls_list:
        detail_url = base_url+url  # 拼接成正确的url视频路径
        detail_index_text = get_datil_index(detail_url) # 返回的是视频详情页html 
        video_detail = get_video(detail_index_text) # 得到的是视频的详细数据
        video_url = video_detail['视频的链接']  
        title = video_detail['标题']
        pool.submit(download_video,video_url,title) # 开线程池将视频保存文件中。
```

## 5. 基于post方法实现登入

```python
"""
经过分析，在登入的时候回，需要写到cookie和token信息，才能登入成功。

1.请求登陆页面 获取token cookie
2.发生登陆的post请求,将用户名密码 和token 放在请求体中,cookie放在请求头中

"""
import requests
import re
login_url = "https://github.com/login"
headers = {"user-agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36"}

res1 = requests.get(login_url,headers=headers)
# 从响应体中获取token
token = re.search('name="authenticity_token" value="(.*?)"',res1.text).group(1)

# 得到cookie的信息
login_cookie = res1.cookies.get_dict()

# 发送登陆请求
res2 = requests.post("https://github.com/session",
              headers={
                  "user-agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36"},
              cookies = login_cookie,
              data={
                "commit": "Sign in",
                "utf8": "✓",
                "authenticity_token": token,
                "login": "yangyuanhu",
                "password": "123654asd"},)

# 用户登录成功后的cookie
user_cookie = res2.cookies.get_dict()

# 访问主页
res3 = requests.get("https://github.com/settings/profile",cookies = user_cookie,headers = headers)
print(res3.status_code)
print(res3.text)
# "https://github.com/settings/profile"
```

## 6. 爬取拉勾网上的信息

```python
import  requests
import  re

import  pyecharts
# 访问你首页的信息
def  get_index():
    response = requests.get(url ='https://www.lagou.com/',

                            headers= {
                                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36'
                            }
                            )
    return  response.text

# 得到各个行业的具体信息

def get_detail(index_text):
     detail_info = re.search('<span>后端开发</span>([\s\S]*?)后端开发其它</a>',index_text).group(1)
     detail_info = detail_info.replace(' ','')
     # 工作的链接及简介：
     ip_list = re.findall('<ahref="(.*?)"data-lg-tj-id=".*?"data-lg-tj-no=".*?"data-lg-tj-cid="idnull"class=".*?">(.*?)</a>',detail_info)
     return  ip_list


# 得到岗位的具体信息
def get_job_detail_info(ip_list):
    lists = ['Java','Python']
    li = []
    for j in range(1,3):
       for ip in ip_list:
           if not ip[1] in lists:
               continue
           response = requests.get(ip[0]+ str(j))
           data = re.findall("""<a class="position_link"([\s\S]*?)<div class="company">""",response.text)

           for i in data:
               dis = {}
               # 岗位的名称
               job_name = re.search('<h3>(.*?)</h3>', i).group(1)
               # 工作地点
               addr = re.search('<em>(.*?)</em>', i).group(1)
               # 发布的时间
               send_time = re.search('<span class="format-time">(.*?)</span>', i).group(1)
               # 薪资
               money = re.search('<span class="money">(.*?)</span>', i).group(1)
               # 工作经验学历
               jy, xl = re.search('<!--<i></i>-->([\s\S].*?)\n', i).group(1).split(' / ')
               if "{" in xl:
                   continue
               dis['岗位的名称'] =job_name
               dis['工作地点'] =addr
               dis['发布的时间'] =send_time
               dis['薪资'] =money
               dis['工作经验'] =jy
               dis['工作学历'] =xl
               dis['工作属性'] =ip[1]

               li.append(dis)
    return li


#得到起薪：
def get_job_money(lis):
    java_count = {}
    python_count = {}
    for data in lis:
        if data['工作属性'] == 'Java':
            last_money = data['薪资'].split('-')[0]
            if last_money in java_count:
                java_count[last_money] += 1
            else:
                java_count[last_money] =1
        else:
            last_money = data['薪资'].split('-')[0]
            if last_money in python_count:
                python_count[last_money] += 1
            else:
                python_count[last_money] = 1

    return  (java_count,python_count)

#展示
def show(job_list,ip_list):
    page = pyecharts.Page()
    for data  in job_list:
        # 创建一个饼图
        pie = pyecharts.Pie()
        # 为饼图添加数据
        """
        标题
        keys
        values
        """
        pie.add(ip_list[1], data.keys(), data.values())
        # 将图加到页面上
        page.add(pie)
    # 生成一个页面文件

    page.render("test.html")


if __name__ == '__main__':
    index_text = get_index()
    ip_list = get_detail(index_text)
    lis = get_job_detail_info(ip_list)
    job_list = get_job_money(lis)
    show(job_list,ip_list)

```

## 7. 实现对抽屉网的大批量点赞和评论

```python
import requests
import re

# 登入：(注之前一定要事先登入上抽屉网,拿到cookie信息)
def login():
    data = requests.get('https://dig.chouti.com/',
                    headers={
                        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36',
                        'cookie': 'gpsd=fb6ad5a24e140a7bddc6d80bb4887c8a; gpid=ec99f9316de344ab96a415a2877f2156; _9755xjdesxxd_=32; YD00000980905869%3AWM_NI=pt33xn5VsCZJ4V%2Fv1I7TnWXGtEMhwIU4IS%2FICCHG%2FtIChLk2qiPd6vvcWjImjBM9D2sXjqzbwMh9qMyAzxfV3iqOrYiykdp6%2FUvR%2BXaTWnNPMMMfDejq0arfm31OkmKNdXQ%3D; YD00000980905869%3AWM_NIKE=9ca17ae2e6ffcda170e2e6eebbd052f5a78298d06fa3bc8ea3c84b879a9b84bc3c98ab9fbbe27eaf8600d0ae2af0fea7c3b92ab4acfdd4fb66f78abda4d942f88ae197b443f58ca2b8db4db89b8389f64397ec87b4ae43819d88a8ae808b9da8adea3ab3b7a085b57eb6bd998bcc67aa9e83dac73ab188a692f96ab593a8daf447bb9fb699e55ef1b7bfb0f153aba7a585f33eed8bbab3aa54a38c998fcd5c8f938387c76faaaf0082f16eafedffb0ea678992acb9d437e2a3; YD00000980905869%3AWM_TID=boNAncLRI%2B1BQBUVEUYt1C7FcIrnNh7%2F; JSESSIONID=aaaR0sMdzd7bz6h_BcFHw; gdxidpyhxdE=hLSAiEurf6pb%2FqzGE%2BZtR8oa3TqDbDsKdxbdvrdxKD8dpp19EaE8ikmOyeONoPsKLXK352UnCrV%2B5U%2B%2FkIk65lzfY%2BfftyXry0LLwJpn2oP6eR50X8gL7KG34KgdQp3Us23le0nbbZ4hg8d0WK3LzsEiIQ%5ChZV0vBhJPLITzwqno%2Bz4i%3A1548210838081; puid=51a253e2d23bca6f78522bd5a5586978; puid=cdu_54820655737'})

    return  data.text


# 过滤出地址
def parmar(data):
    li =[]
    data = data.replace("\n" ,'')
    info = re.findall('<div class="item">[\s\S]*? share-linkid="(.*?)" share-subject=".*?">',data)
    for i in info:
        url ='https://dig.chouti.com/link/vote?linksId='+i
        li.append(url)
    return  (li,info)


# 实现点赞
def up(ip_list):
    for i in ip_list:
        response = requests.post(url= i,
                                 headers ={
                                     'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36',
                                      'referer': 'https://dig.chouti.com/',
                                      'cookie':'gpsd=fb6ad5a24e140a7bddc6d80bb4887c8a; gpid=ec99f9316de344ab96a415a2877f2156; _9755xjdesxxd_=32; YD00000980905869%3AWM_NI=pt33xn5VsCZJ4V%2Fv1I7TnWXGtEMhwIU4IS%2FICCHG%2FtIChLk2qiPd6vvcWjImjBM9D2sXjqzbwMh9qMyAzxfV3iqOrYiykdp6%2FUvR%2BXaTWnNPMMMfDejq0arfm31OkmKNdXQ%3D; YD00000980905869%3AWM_NIKE=9ca17ae2e6ffcda170e2e6eebbd052f5a78298d06fa3bc8ea3c84b879a9b84bc3c98ab9fbbe27eaf8600d0ae2af0fea7c3b92ab4acfdd4fb66f78abda4d942f88ae197b443f58ca2b8db4db89b8389f64397ec87b4ae43819d88a8ae808b9da8adea3ab3b7a085b57eb6bd998bcc67aa9e83dac73ab188a692f96ab593a8daf447bb9fb699e55ef1b7bfb0f153aba7a585f33eed8bbab3aa54a38c998fcd5c8f938387c76faaaf0082f16eafedffb0ea678992acb9d437e2a3; YD00000980905869%3AWM_TID=boNAncLRI%2B1BQBUVEUYt1C7FcIrnNh7%2F; JSESSIONID=aaaR0sMdzd7bz6h_BcFHw; puid=cdu_54820655737; puid=a5beaabe3367e589703376cb89636d1a; gdxidpyhxdE=fOqKYKO%2FtVvbDNno087Rc2rn7KqTA0%5CbL%2BeuZqv4QcU0zCPuJlBTcR5eqXGK10IQXLrcH2ju%2FE2RPf8RTPVLH0GE2zA%5CaJfCOcP0TOgg2qrI4jX1dd4jSuziWJWsEzwMcSnayLlcT%5Cs%5CkNVt93OV2uZW6RWx5RKqRWPQG1lihy3aJJd7%3A1548213358293'
                                 }
                                 )
        print('点赞成功')
        

# 实现评论
def commit(info):
    for i in info:
        response = requests.post(url = 'https://dig.chouti.com/comments/create',
                             headers={
                                 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36',
                                 'referer': 'https://dig.chouti.com/',
                                 'cookie': 'gpsd=fb6ad5a24e140a7bddc6d80bb4887c8a; gpid=ec99f9316de344ab96a415a2877f2156; _9755xjdesxxd_=32; YD00000980905869%3AWM_NI=pt33xn5VsCZJ4V%2Fv1I7TnWXGtEMhwIU4IS%2FICCHG%2FtIChLk2qiPd6vvcWjImjBM9D2sXjqzbwMh9qMyAzxfV3iqOrYiykdp6%2FUvR%2BXaTWnNPMMMfDejq0arfm31OkmKNdXQ%3D; YD00000980905869%3AWM_NIKE=9ca17ae2e6ffcda170e2e6eebbd052f5a78298d06fa3bc8ea3c84b879a9b84bc3c98ab9fbbe27eaf8600d0ae2af0fea7c3b92ab4acfdd4fb66f78abda4d942f88ae197b443f58ca2b8db4db89b8389f64397ec87b4ae43819d88a8ae808b9da8adea3ab3b7a085b57eb6bd998bcc67aa9e83dac73ab188a692f96ab593a8daf447bb9fb699e55ef1b7bfb0f153aba7a585f33eed8bbab3aa54a38c998fcd5c8f938387c76faaaf0082f16eafedffb0ea678992acb9d437e2a3; YD00000980905869%3AWM_TID=boNAncLRI%2B1BQBUVEUYt1C7FcIrnNh7%2F; JSESSIONID=aaaR0sMdzd7bz6h_BcFHw; puid=cdu_54820655737; puid=a5beaabe3367e589703376cb89636d1a; gdxidpyhxdE=fOqKYKO%2FtVvbDNno087Rc2rn7KqTA0%5CbL%2BeuZqv4QcU0zCPuJlBTcR5eqXGK10IQXLrcH2ju%2FE2RPf8RTPVLH0GE2zA%5CaJfCOcP0TOgg2qrI4jX1dd4jSuziWJWsEzwMcSnayLlcT%5Cs%5CkNVt93OV2uZW6RWx5RKqRWPQG1lihy3aJJd7%3A1548213358293'
                             },
                             data ={
                                 'jid': 'cdu_54820655737',
                                 'linkId': i,
                                 'content': '写的真好',
                                 'sortType': 'score',
                             }
                             )
        print('%s:评论成功'%i)





if __name__ == '__main__':
    # 登入，得到首页的信息
    data = login()
    # 得到需要的数据
    li,info= parmar(data)
    # 实现点赞
    up(li)
    # 评论
    commit(info)



```

