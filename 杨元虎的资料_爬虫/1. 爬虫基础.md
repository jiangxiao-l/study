# 爬虫简介

### 什么是爬虫

​	爬虫是一种应用程序，用于从互联网中获取有价值的数据，从本质上来看，属于client客户端程序。

### 互联网简介

​	互联网是由各种计算机设备，通过连接介质相互连接而组成的，其目的就是为了能在不同计算机之间传输数据，并且在互联网上有大量的数据是免费的。如果没有互联网，你只能拿着u盘过去拷贝。。

### 爬虫的原理

​	通常我们所谓的上网，其实本质就是用计算机通过网络去访问另一台计算机上的数据，而这些数据通常以网页的形式存在于服务器上，网页本质上就是一个文本文件，要想得到有价值的数据，第一步就是要得到这些网页数据，这就需要**分析浏览器与服务器之间到底是如何通讯的，然后模拟浏览器来与服务器通讯，从而获取数据。**当然爬虫不仅限于爬取网页，还可爬取移动设备常用的json数据等等，以及其他格式的二进制数据等等。

### 爬虫的价值

​	互联网中最宝贵的就是数据了，例如淘宝的商品数据，链家的房源信息，拉钩的招聘信息等等，这些数据就像一座矿山，爬虫就像是挖矿的工具，掌握了爬虫技术，你就成了矿山老板，各网站都在为你免费提供数据。

### 爬虫的爬取过程

​	分析--模拟请求--解析数据--存储

# HTTP请求分析

首先要明确的是：爬虫的核心原理就是模拟浏览器发送HTTP协议来获取服务器上的数据，那么要想服务器接受你的请求，则必须将自己的请求伪装的足够像，这就需要我们去分析浏览器是如何发送的HTTP请求

其次：HTTP协议是基于请求响应模型的，客户端发送请求到服务器，服务器接受请求，处理后返回响应数据，需要关注的重点在于请求数据，只有服务器认为合格合法的请求才会得到服务器的响应。

### 利用chrome开发者工具来分析请求

chrome浏览器提供强大的开发者工具我们可以利用它来查看浏览器与服务器的整个通讯过程

![image-20190114194039219](https://ws1.sinaimg.cn/large/006tNc79ly1fz6ckp34m4j313u0o9n3z.jpg)

上图划出了常用的几个操作，如清空请求，保留日志等等，另一个常用操作就是清空cookie信息

打开chrome的设置页面搜索cookie就可以找到清空按钮。

![image-20190120201342706](https://ws3.sinaimg.cn/large/006tNc79gy1fzdb8vix2cj30dw0df0tw.jpg)

## 请求流程分析

#### 请求地址

​	浏览器发送的请求URL地址

#### 请求方法

​	get  中文需要URL编码，参数跟在地址后面

​	post 参数放在body中

#### 请求头

![image-20190120195336647](https://ws4.sinaimg.cn/large/006tNc79gy1fzdanz4zvvj30l508swgv.jpg)

​	cookie，需要登录成功才能访问的页面就需要传递cookie，否则则不需要cookie

​	user-agent，用户代理，验证客户端的类型

​	referer，引用页面，判断是从哪个页面点击过来的

#### 请求体

![image-20190120200016406](https://ws3.sinaimg.cn/large/006tNc79gy1fzdauwhqu1j30ct04gq3i.jpg)

​	只在post请求时需要关注，通常post请求参数都放在请求体中，例如登录时的用户名和密码

#### 响应头

![image-20190120195155247](https://ws1.sinaimg.cn/large/006tNc79gy1fzdam9rejqj30jw05cdgx.jpg)

​	**location**：重定向的目标地址，仅 在状态码为3XX时出现，需要考虑重定向时的方法，参数等。。，浏览器会自动重定向，request模块也会。

​	**set-cookie**：服务器返回的cookie信息，在访问一些隐私页面是需要带上cookie

#### 响应体

​	服务器返回的数据，可能以下几种类型

​	**HTML格式的静态页面**		需要解析获取需要的数据

​	**json个格式的结构化数据**	直接就是纯粹的数据

​	 **二进制数据**				通过文件操作直接写入文件



# 爬取梨视频

​	

# request模块的使用

## get请求

#### 请求参数

1.参数拼接时中文需要URLEncode，可以使用urllib中的编码函数来完成

```python
from urllib.parse import urlencode
import requests

params = {"wd":"美女"}
# 必须以字典类型传入需要编码的数据 得到拼接收的参数字符串
res = urlencode(params,encoding="utf-8")

url = "https://www.baidu.com/s"
url = url+"?"+res # 拼接参数

response = requests.get(url)

with open("test.html","wb") as f:
    f.write(response.content)
```

2.也可以直接将请求参数传给get的函数的params参数，get方法会自动完成编码

```python
import requests
# 参数字典
params = {"wd":"美女"}

url = "https://www.baidu.com/s"
# 直接传给params参数
response = requests.get(url,params=params)

with open("test.html","wb") as f:
    f.write(response.content)
```

上述的代码无法请求完成的数据 因为百度服务器有限制，必须制定用户代理，user-agent为浏览器才行

#### 添加请求头

```python
import requests

params = {"wd":"美女"}
url = "https://www.baidu.com/s"
headers = {
"user-agent" : 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Mobile Safari/537.36'
}
response = requests.get(url,params=params,headers=headers)

with open("test.html","wb") as f:
```

#### cookies

可以在headers中直接添加键值对也可以单独使用get方法的cookies参数

```python
url = "https://github.com/yangyuanhu"
headers = {
    "User-Agent" : 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Mobile Safari/537.36',
    "Cookie":"_octo=GH1.1.1155792533.1532919557; _ga=GA1.2.1847433334.1534242542; has_recent_activity=1; user_session=Z5AQmC_Wv4wvM-_Nc3Bj0PQn6nITSonDcPkw4GZ1g0jFIqbe; __Host-user_session_same_site=Z5AQmC_Wv4wvM-_Nc3Bj0PQn6nITSonDcPkw4GZ1g0jFIqbe; logged_in=yes; dotcom_user=yangyuanhu; tz=Asia%2FShanghai; _gat=1; _gh_sess=eERwWHk1NVBjOEhIRmxzckcrcWlpVCtUM2hWL0prNlIyNXdTdC9ab0FSZWR5MEZlYW5OVGNoTEdJQUN0K0xkNXVmTEVpK2RRU1VZTUF6NkJHM1JMejNtaVhReXg2M3Vsb0JvcG8vNDNENjNBcXVvcFE4Tmp4SFhpQ3Z3S2ZZZEIwTGNkVW5BR01qVHlCNEFqTnZFcllhN3NwT1VJblZYWElLOGowN3ZaZVpZTFVHQlFtZkh3K1hnclBWalB1Mk1XLS1wNjFjMlBxUHljU2paK2RXU3M5eDB3PT0%3D--8cdf657174d2acac7cc0459da667accc8a2d0f5e",
    "Referer": "https://github.com/"
}
response = requests.get(url,headers=headers)
with open("test.html","wb") as f:
    f.write(response.content)

# 使用cookies参数  注意需要拆分为单独的键值对
#response = requests.get(url,headers=headers,cookies={"_octo":"GH1.1.1155792533.1532919557"})
```

## POST请求

模拟登录流程并爬取github私有页面



